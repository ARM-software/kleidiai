#
# SPDX-FileCopyrightText: Copyright 2024-2025 Arm Limited and/or its affiliates <open-source-office@arm.com>
#
# SPDX-License-Identifier: Apache-2.0
#

stages:
  - build
  - test
  - analyze
  - deploy

variables:
  # Set of KUBERNETES_* variables applies only to dynamic EKS Linux cluster and do not work
  # with other runners
  #
  # KUBERNETES_NODE_SELECTOR_ARCH selects architecture for job container.
  # For arm64, it should be: 'kubernetes.io/arch=arm64'. This is default if possible
  # For amd64 it should be: 'kubernetes.io/arch=amd64'
  KUBERNETES_NODE_SELECTOR_ARCH: "kubernetes.io/arch=arm64"

  # Define what type of node to use (on-demand or spot). Spot is recommended and is MUCH less expensive than on-demand.
  # But it can be reclaimed by cloud at any time causing the pod to lose the node.
  # on-demand is used for a longer runs to avoid retries
  KUBERNETES_NODE_SELECTOR_TYPE: "karpenter.sh/capacity-type=spot"

  # Define what generation of node to use
  # For performance nodes - use 'karpenter.k8s.aws/instance-generation=8'
  #   faster execution for jobs relying on CPU like compilation or on-host testing
  #   and results in lower total cost
  #
  # For cost-efficient nodes - use 'karpenter.k8s.aws/instance-generation=7'
  #   slower execution, but could be used for jobs waiting for different kind of
  #   remote infrastructure like on-device testing or accessing servers
  #
  KUBERNETES_NODE_SELECTOR_INSTANCE_GENERATION: "karpenter.k8s.aws/instance-generation=8"

  # KUBERNETES_CPU_REQUEST reserves the amount CPUs on a node. The amount is guaranteed for a container
  KUBERNETES_CPU_REQUEST: 4
  # KUBERNETES_CPU_LIMIT limits maximum limit of CPU the job container can use. This is not reserved or guaranteed
  KUBERNETES_CPU_LIMIT: 6

  # KUBERNETES_MEMORY_REQUEST reserves the amount memory on a node. The amount is guaranteed for a container
  KUBERNETES_MEMORY_REQUEST: 4Gi
  # KUBERNETES_MEMORY_LIMIT limits maximum limit of memory the job container can use. This is not reserved or guaranteed
  KUBERNETES_MEMORY_LIMIT: 6Gi

  # This variable defines a number of parallel jobs (-j) which should be used.
  # We could not rely on `nproc` result as it would return number of ALL host CPUs, not one reserved.
  # And this might cause huge memory usage and OOM of container
  PARALLEL_JOBS: ${KUBERNETES_CPU_REQUEST}

  # Enable more verbose script section content printout
  FF_SCRIPT_SECTIONS: true

default:
  image: registry.gitlab.arm.com/kleidi/kleidiai/image:latest
  tags:
    - arm64
  interruptible: true
  retry:
    max: 2
    when:
      - job_execution_timeout
      - stuck_or_timeout_failure
      - runner_system_failure

.standard-rules:
  timeout: 30m
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_TAG && $CI_COMMIT_REF_PROTECTED == "true"

workflow:
  auto_cancel:
    on_new_commit: interruptible

build-clang:
  extends:
    - .standard-rules
  stage: build
  script:
    - cmake -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_CXX_FLAGS="-Werror" -DCMAKE_C_FLAGS="-Werror" -DCMAKE_BUILD_TYPE=Release -DKLEIDIAI_BUILD_TESTS=ON -S . -B ${CI_JOB_NAME_SLUG}
    - cmake --build ${CI_JOB_NAME_SLUG} -j${PARALLEL_JOBS} --verbose
  artifacts:
    expire_in: 1 day
    paths:
      - ${CI_JOB_NAME_SLUG}/kleidiai_test

build-gcc:
  extends:
    - .standard-rules
  stage: build
  script:
    - cmake -G Ninja -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DCMAKE_CXX_FLAGS="-Werror" -DCMAKE_C_FLAGS="-Werror" -DCMAKE_BUILD_TYPE=Release -DKLEIDIAI_BUILD_TESTS=ON -S . -B ${CI_JOB_NAME_SLUG}
    - cmake --build ${CI_JOB_NAME_SLUG} -j${PARALLEL_JOBS} --verbose
  artifacts:
    expire_in: 1 day
    paths:
      - ${CI_JOB_NAME_SLUG}/kleidiai_test

build-gcc-bazel:
  extends:
    - .standard-rules
  stage: build
  cache:
    - key: cache-bazelisk
      paths:
      - /cache/bazelisk
  script:
    - bazelisk clean
    - bazelisk build -c opt --copt="-Werror" --cxxopt="-Werror" --jobs=${PARALLEL_JOBS} -k --subcommands --verbose_failures --curses=no //...
    - mkdir -p ${CI_JOB_NAME_SLUG} && cp bazel-bin/test/kleidiai_test ${CI_JOB_NAME_SLUG}/
  artifacts:
    expire_in: 1 day
    paths:
      - ${CI_JOB_NAME_SLUG}/kleidiai_test

build-clang-bazel:
  extends:
    - .standard-rules
  stage: build
  cache:
    - key: cache-bazelisk
      paths:
      - /cache/bazelisk
  script:
    - bazelisk clean
    # explicitly disable layering_check feature
    - CC=clang bazelisk build -c opt --copt="-Werror" --cxxopt="-Werror" --jobs=${PARALLEL_JOBS} -k --subcommands --verbose_failures --compiler=clang --features=no-layering_check --curses=no //...
    - mkdir -p ${CI_JOB_NAME_SLUG} && cp bazel-bin/test/kleidiai_test ${CI_JOB_NAME_SLUG}/
  artifacts:
    expire_in: 1 day
    paths:
      - ${CI_JOB_NAME_SLUG}/kleidiai_test

build-examples:
  stage: build
  extends:
    - .standard-rules
  script:
    - mkdir -p build
    - >
      for EXAMPLE in `ls examples -1`; do
        if [ -f examples/${EXAMPLE}/CMakeLists.txt ]; then
          echo "-----------------------------------------------------------"
          echo "Build examples/${EXAMPLE}"
          echo "-----------------------------------------------------------"
          mkdir -p build_${EXAMPLE}
          cmake -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_CXX_FLAGS="-Werror" -DCMAKE_C_FLAGS="-Werror" -DCMAKE_BUILD_TYPE=Release -S examples/$EXAMPLE -B build_${EXAMPLE}
          cmake --build build_${EXAMPLE} -j${PARALLEL_JOBS} --verbose
          cp build_${EXAMPLE}/${EXAMPLE} build/
        else
          echo "No build file found for ${EXAMPLE}"
          exit 1
        fi
      done
  artifacts:
    expire_in: 1 day
    paths:
      - build

test-examples:
  stage: test
  extends:
    - .standard-rules
  needs:
    - build-examples
  script:
    - >
      for EXAMPLE in `ls build -1`; do
          [[ $EXAMPLE == *sme* ]] && continue
          echo "-----------------------------------------------------------"
          echo "Run ${EXAMPLE}"
          echo "-----------------------------------------------------------"
          build/${EXAMPLE} | tee -a example_${EXAMPLE}.log
      done
  artifacts:
    expire_in: 1 day
    paths:
      - "example_*.log"

test-clang-tidy:
  extends:
    - .standard-rules
  stage: test
  needs: []
  script:
    - cmake -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_CXX_FLAGS="-Werror" -DCMAKE_C_FLAGS="-Werror" -DCMAKE_BUILD_TYPE=Release -DKLEIDIAI_BUILD_TESTS=ON -DKLEIDIAI_BUILD_BENCHMARK=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -B build/${CI_JOB_NAME_SLUG}
    # Only test the main library.
    # Only test C/C++ files.
    - >
        clang-tidy --header-filter ".*" --warnings-as-errors "*" -p build/${CI_JOB_NAME_SLUG} $(find kai -type f \( -name \*.c -o -name \*.cpp \))

pre-commit-hooks:
  variables:
    PRE_COMMIT_HOME: '/cache/pre-commit'
  extends:
    - .standard-rules
  stage: build
  cache:
    - key: cache-pre-commit
      paths:
      - $PRE_COMMIT_HOME
  script:
    - PRE_COMMIT_HOME=$PRE_COMMIT_HOME pre-commit run --all-files

test-remote:
  # Part of the pipeline is executed in a separate system.
  #
  # When the remote pipeline has been completed, this job is manually triggered
  # with the information about the remote pipeline including whether it's passed or failed.
  # Run the job only for a public pipeline
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event' && $CI_SERVER_HOST == 'gitlab.arm.com'
  # Longer timeout than rest of jobs to account for remote
  timeout: 1h
  stage: test
  needs: []
  when: manual
  allow_failure: false
  variables:
    REMOTE_PIPELINE_ID: 0
    REMOTE_PIPELINE_PASSED: ""
    REMOTE_PIPELINE_MESSAGE: ""
  script:
    - echo "REMOTE_PIPELINE_ID=${REMOTE_PIPELINE_ID}" |& tee remote_result.txt
    - echo "REMOTE_PIPELINE_PASSED=${REMOTE_PIPELINE_PASSED}" |& tee -a remote_result.txt
    - echo "REMOTE_PIPELINE_MESSAGE=${REMOTE_PIPELINE_MESSAGE}" |& tee -a remote_result.txt
    - echo "${REMOTE_PIPELINE_PASSED}" | grep -q "true"
  artifacts:
    when: always
    expire_in: 1 day
    paths:
      - remote_result.txt

# Run unit tests
# Input variables:
# * UT_VARIANT_NAME - base name used to find path to unit test binaries and generate report name
# * UT_PARALLEL - number of separate executions, default 4 since 100MB / 30MB = 3.(3) and rounded up
# * UT_BINARY_PATH - path to folder with unit test binaries, default is ./build-${UT_VARIANT_NAME}/
# * UT_REPORT_PATH - prefix to unit test report file path, default is ./kleidiai_results-${UT_VARIANT_NAME}
# * UT_EXTRA_ARGS - extra arguments for test binary, default is none
.run-unit-tests: &run-unit-tests
  - echo "Run unit test from ${UT_BINARY_PATH:-./build-${UT_VARIANT_NAME}}"
  - export GTEST_TOTAL_SHARDS=${UT_PARALLEL:-4}
  - |
    for GTEST_SHARD_INDEX in `seq 0 $((GTEST_TOTAL_SHARDS - 1))` ; do
      export GTEST_SHARD_INDEX
      echo "Running iteration $GTEST_SHARD_INDEX of $GTEST_TOTAL_SHARDS"
      ${UT_BINARY_PATH:-./build-${UT_VARIANT_NAME}}/kleidiai_test --gtest_brief=1 --gtest_output=xml:${UT_REPORT_PATH:-kleidiai_results-${UT_VARIANT_NAME}}${GTEST_SHARD_INDEX}.xml ${UT_EXTRA_ARGS:-}
    done

test-linux-aarch64:
  extends:
    - .standard-rules
  stage: test
  parallel:
    matrix:
      - BUILD_JOB_PROVIDER: [ clang, gcc, clang-bazel, gcc-bazel ]
  needs:
    - build-gcc
    - build-gcc-bazel
    - build-clang
    - build-clang-bazel
  script:
    - |
      UT_VARIANT_NAME=$BUILD_JOB_PROVIDER
      UT_REPORT_PATH=kleidiai_test_results-${BUILD_JOB_PROVIDER}
      echo "UT_REPORT_PATH=$UT_REPORT_PATH" >> $GITLAB_ENV
    - *run-unit-tests
  artifacts:
    when: always
    expire_in: 1 day
    paths:
      - ${UT_REPORT_PATH}*.xml
    reports:
      junit: ${UT_REPORT_PATH}*.xml

# Script to run binary provided in FVP environment
# * FVP_TEST_EXECUTABLE - mandatory path in CI for file and parameters to execute
# * FVP_MODEL_EXTRA - optional additional FVP parameters, default is nothing
# * FVP_HOST_PATH - optional path to share via P9 FS, default is $PWD
.test-linux-fvp: &test-linux-fvp
  - if [ ! -f /opt/devtools/linux-rootfs.img ]; then tar xJf /opt/devtools/linux-rootfs.img.xz; fi
  - >
    echo "#!/bin/bash
      set -ex
      echo '=================================================='
      echo '== START                                        =='
      echo '=================================================='
      echo '== CPU INFO                                     =='
      if [ ! -f /proc/cpuinfo ]; then mount -vt proc -o rw,nosuid,nodev,noexec proc /proc; fi
      cat /proc/cpuinfo
      echo '=================================================='

      mkdir -vp '$PWD'
      mount -vt 9p -o trans=virtio,version=9p2000.L FM '$PWD'

      cd '$PWD'
      ${FVP_TEST_EXECUTABLE} && echo 'FINISHED WITHOUT ERROR'
      sync

      echo '=================================================='
      echo '== END                                          =='
      echo '=================================================='
      " > startup
  - e2cp -O 0 -G 0 -P 755 startup /opt/devtools/linux-rootfs.img:/root/startup
  - >
    /opt/devtools/fvp_base_aemva/models/Linux64*GCC-9.3/FVP_Base_RevC-2xAEMvA \
      -C cache_state_modelled=0 \
      -C bp.refcounter.non_arch_start_at_default=1 \
      -C bp.secure_memory=0 \
      -C bp.pl011_uart0.out_file=- \
      -C bp.pl011_uart0.shutdown_tag="System halted" \
      -C bp.terminal_0.mode=telnet \
      -C bp.terminal_0.start_telnet=0 \
      -C bp.terminal_1.mode=raw \
      -C bp.terminal_1.start_telnet=0 \
      -C bp.terminal_2.mode=raw \
      -C bp.terminal_2.start_telnet=0 \
      -C bp.terminal_3.mode=raw \
      -C bp.terminal_3.start_telnet=0 \
      -C pctl.startup=*.*.*.* \
      -C cluster1.NUM_CORES=0 \
      -C cluster0.NUM_CORES=2 \
      -C cluster0.has_arm_v8-1=1 \
      -C cluster0.has_arm_v8-2=1 \
      -C cluster0.has_arm_v8-3=1 \
      -C cluster0.has_arm_v8-4=1 \
      -C cluster0.has_arm_v8-5=1 \
      -C cluster0.has_arm_v8-6=1 \
      -C cluster0.has_arm_v8-7=1 \
      -C cluster0.has_arm_v8-8=1 \
      -C cluster0.has_arm_v9-0=1 \
      -C cluster0.has_arm_v9-1=1 \
      -C cluster0.has_arm_v9-2=1 \
      -C cluster0.has_arm_v9-3=1 \
      -C cluster0.has_arm_v9-4=1 \
      -C cluster0.has_arm_v9-5=1 \
      -C cluster0.has_sve=1 \
      -C cluster0.sve.has_b16b16=1 \
      -C cluster0.sve.has_sve2=1 \
      -C cluster0.sve.has_sme=1 \
      -C cluster0.sve.has_sme2=1 \
      -C cluster0.sve.has_sme_f16f16=1 \
      -C cluster0.sve.has_sme_fa64=1 \
      -C cluster0.sve.has_sme_lutv2=1 \
      -C cluster0.sve.sme2_version=1 \
      -C cluster0.sve.veclen=2 \
      -C cluster0.sve.sme_veclens_implemented=4 \
      -C bp.virtio_rng.enabled=1 \
      -C bp.virtioblockdevice.image_path=/opt/devtools/linux-rootfs.img \
      -C bp.vis.disable_visualisation=1 \
      -C bp.virtiop9device.root_path=${FVP_HOST_PATH:-$PWD} \
      -a cluster*.cpu*=/opt/devtools/linux-system.axf \
      ${FVP_MODEL_EXTRA:-} \
      |& tee output.txt
  - grep -q "FINISHED WITHOUT ERROR" output.txt

.test-linux-aarch64-fvp-nosve:
  extends:
    - .standard-rules
  stage: test
  parallel:
    matrix:
      - BUILD_JOB_PROVIDER: [ clang, gcc, clang-bazel, gcc-bazel ]
  needs:
    - build-gcc
    - build-gcc-bazel
    - build-clang
    - build-clang-bazel
  variables:
    # A long running test job
    KUBERNETES_NODE_SELECTOR_TYPE: "karpenter.sh/capacity-type=on-demand"
    FVP_MODEL_EXTRA: "-C cluster0.sve.has_sve2=0 -C cluster0.sve.sme_only=1"
    FVP_TEST_EXECUTABLE: "./build-${BUILD_JOB_PROVIDER}/kleidiai_test --gtest_output=xml:kleidiai_test_results-${BUILD_JOB_PROVIDER}.xml"
  script:
    - *test-linux-fvp
  artifacts:
    when: always
    expire_in: 1 day
    paths:
      - kleidiai_test_results-${BUILD_JOB_PROVIDER}.xml
    reports:
      junit: kleidiai_test_results-${BUILD_JOB_PROVIDER}.xml

test-linux-aarch64-v8only-fvp:
  extends:
    - .standard-rules
  stage: test
  needs:
    - build-clang
    - build-examples
  script:
    - >
      FVP_MODEL_EXTRA="
        -C cluster0.has_arm_v8-1=0
        -C cluster0.has_arm_v8-2=0
        -C cluster0.has_arm_v8-3=0
        -C cluster0.has_arm_v8-4=0
        -C cluster0.has_arm_v8-5=0
        -C cluster0.has_arm_v8-6=0
        -C cluster0.has_arm_v8-7=0
        -C cluster0.has_arm_v8-8=0
        -C cluster0.has_arm_v9-0=0
        -C cluster0.has_arm_v9-1=0
        -C cluster0.has_arm_v9-2=0
        -C cluster0.has_arm_v9-3=0
        -C cluster0.has_arm_v9-4=0
        -C cluster0.has_arm_v9-5=0
        -C cluster0.has_sve=0"
    - |
      echo -e "#\!/bin/bash -ex" > run_tests.sh
      echo -e "echo \"Perform a positive test where advanced instructions feature detected\"" >> run_tests.sh
      echo "./build-clang/kleidiai_test --gtest_brief=1 || exit 1" >> run_tests.sh
      echo -e "echo \"Perform a negative test with illegal instructions\"" >> run_tests.sh
      for EXAMPLE in `ls -1 build`; do
          echo -e "echo \"Run ${EXAMPLE}\"\n./build/${EXAMPLE} || true" >> run_tests.sh
      done
      chmod a+x run_tests.sh
    - FVP_TEST_EXECUTABLE="./run_tests.sh"
    - *test-linux-fvp
    # Verify that no extra features enabled and CPU variant is v8.0
    - "grep -qE 'Features\\s+: fp asimd evtstrm cpuid' output.txt"
    - "grep -qE 'CPU variant\\s+: 0' output.txt"
    # Verify that all examples crashed with Illegal instruction
    - test $(grep -cE 'Illegal instruction\s+./build/' output.txt) -eq $(ls -1 build | wc -l | tr -d ' ')

test-linux-aarch64-sme1only-fvp:
  extends:
    - .standard-rules
  variables:
    # A long running test job
    KUBERNETES_NODE_SELECTOR_TYPE: "karpenter.sh/capacity-type=on-demand"
  timeout: 2h
  stage: test
  needs:
    - build-clang
  script:
    - >
      FVP_MODEL_EXTRA="
        -C cluster0.sve.has_sme2=0
        -C cluster0.sve.sme2_version=0"
    - |
      echo -e "#\!/bin/bash -ex" > run_tests.sh
      echo -e "echo \"Perform a positive test where advanced instructions feature detected\"" >> run_tests.sh
      echo "./build-clang/kleidiai_test --gtest_output=xml:kleidiai-${CI_JOB_NAME_SLUG}.xml --gtest_brief=1 --gtest_filter=*sme_* || exit 1" >> run_tests.sh
      echo -e "echo \"Perform a negative test with illegal instructions\"" >> run_tests.sh
      for EXAMPLE in `ls -1 build`; do
          echo -e "echo \"Run ${EXAMPLE}\"\n./build/${EXAMPLE} || true" >> run_tests.sh
      done
      chmod a+x run_tests.sh
    - FVP_TEST_EXECUTABLE="./run_tests.sh"
    - *test-linux-fvp
    # Verify that SME2 is not available.
    - "grep -qv 'sme2' output.txt"
  artifacts:
    expire_in: 1 day
    paths:
      - kleidiai-${CI_JOB_NAME_SLUG}.xml
    reports:
      junit: kleidiai-${CI_JOB_NAME_SLUG}.xml

test-examples-sme-fvp:
  extends:
    - .standard-rules
  stage: test
  needs:
    - build-examples
  script:
    - |
      echo -e "#\!/bin/bash -ex" | tee run_tests.sh
      for EXAMPLE in `ls -1 build`; do
        [[ $EXAMPLE == *sme* ]] || continue
        echo "-----------------------------------------------------------" | tee run_tests.sh
        echo "Run ${EXAMPLE}" | tee run_tests.sh
        echo "-----------------------------------------------------------" | tee run_tests.sh
        echo -e "echo \"Run ${EXAMPLE}\"\n./build/${EXAMPLE} | tee -a example_${EXAMPLE}.log" | tee run_tests.sh
      done
      chmod a+x run_tests.sh
    - FVP_TEST_EXECUTABLE="./run_tests.sh"
    - *test-linux-fvp
  artifacts:
    expire_in: 1 day
    paths:
      - "example_*.log"

coverage-clang:
  extends:
    - .standard-rules
  stage: analyze
  needs: []
  tags:
    - apple_silicon
  script:
    # Configure and build
    - cmake -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++
      -DCMAKE_CXX_FLAGS="-Werror --coverage" -DCMAKE_C_FLAGS="-Werror --coverage" -DCMAKE_BUILD_TYPE=Release
      -DKLEIDIAI_BUILD_TESTS=ON -DKLEIDIAI_BUILD_BENCHMARK=ON -S . -B ${CI_JOB_NAME_SLUG}
    - cmake --build ${CI_JOB_NAME_SLUG} -j${PARALLEL_JOBS} --verbose
    # Run kleidiai_test to generate coverage report for unit tests
    - ./${CI_JOB_NAME_SLUG}/kleidiai_test --gtest_brief=1
    # Run kleidiai_kleidiai_benchmark to generate coverage report for benchmark
    - ./${CI_JOB_NAME_SLUG}/kleidiai_benchmark --benchmark_dry_run=true -m 1 -n 30 -k 64
    # Generate gcovr configuration
    - mkdir -p build/coverage
    - |
      echo -e "
      root=${PWD}
      exclude=${PWD}/${CI_JOB_NAME_SLUG}
      gcov-ignore-parse-errors=suspicious_hits.warn
      gcov-executable=xcrun llvm-cov gcov
      exclude-unreachable-branches=yes
      exclude-lines-by-pattern=.*KAI_(?:ASSERT|ASSUME|ERROR).*
      exclude-branches-by-pattern=.*KAI_(?:ASSERT|ASSUME).*" > build/gcovr.cfg
    # Post-process generated coverage report
    - gcovr --json=build/coverage/${CI_JOB_NAME_SLUG}.json -j ${PARALLEL_JOBS} --config build/gcovr.cfg
    # Generage Cobertura and HTML reports
    - mkdir -p build/html/coverage
    - gcovr --json-add-tracefile "build/coverage/*.json" --print-summary
      --cobertura=build/coverage.xml --html-details=build/html/coverage/coverage_report.html
      --html-title="KleidiAI Coverage Report" -j ${PARALLEL_JOBS}
  coverage: '/^lines:\s+(\d+.\d)%\s+/'
  artifacts:
    name: ${CI_JOB_NAME}-${CI_COMMIT_REF_NAME}-${CI_COMMIT_SHA}
    expire_in: 1 day
    reports:
      coverage_report:
        coverage_format: cobertura
        path: build/coverage.xml
    paths:
      - build

pages:
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  timeout: 30m
  stage: deploy
  tags:
    - apple_silicon
  needs:
    - coverage-clang
  script:
    - |
      echo '
        <!DOCTYPE html>
        <html>
          <head>
            <meta http-equiv="refresh" content="5; url=coverage/coverage_report.html" />
            <title>Redirecting...</title>
          </head>
          <body>
            <p>If you are not redirected soon, <a href="coverage/coverage_report.html">click here</a>.</p>
          </body>
        </html>' > build/html/index.html
  artifacts:
    paths:
      - build/html
  publish: build/html
